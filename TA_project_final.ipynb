{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERdJWsO8u-RF"
      },
      "outputs": [],
      "source": [
        "import json, re, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "import tensorflow as tf\n",
        "import keras.utils\n",
        "if not hasattr(keras.utils, \"unpack_x_y_sample_weight\"):\n",
        "    keras.utils.unpack_x_y_sample_weight = tf.keras.utils.unpack_x_y_sample_weight\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
        "    Bidirectional, LSTM, MultiHeadAttention, LayerNormalization,\n",
        "    GlobalAveragePooling1D\n",
        ")\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from tensorflow.keras.optimizers import legacy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Load & Prepare Dataset\n",
        "data = [json.loads(line) for line in open(\"/content/Sarcasm_Headlines_Dataset.json\", \"r\")]\n",
        "df = pd.DataFrame(data)[[\"headline\", \"is_sarcastic\"]]\n",
        "print(df['headline'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyT1lQVevAca",
        "outputId": "d41ed23d-ceae-4ea3-93bd-e772e95c8f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        former versace store clerk sues over secret 'b...\n",
            "1        the 'roseanne' revival catches up to our thorn...\n",
            "2        mom starting to fear son's web series closest ...\n",
            "3        boehner just wants wife to listen, not come up...\n",
            "4        j.k. rowling wishes snape happy birthday in th...\n",
            "                               ...                        \n",
            "26704                 american politics in moral free-fall\n",
            "26705                              america's best 20 hikes\n",
            "26706                                reparations and obama\n",
            "26707    israeli ban targeting boycott supporters raise...\n",
            "26708                    gourmet gifts for the foodie 2014\n",
            "Name: headline, Length: 26709, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return re.sub(r\"[^a-zA-Z']\", \" \", text).lower()\n",
        "\n",
        "df[\"headline\"] = df[\"headline\"].apply(clean_text)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"headline\"], df[\"is_sarcastic\"], test_size=0.2, random_state=42)\n",
        "print(df['headline'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVU6CHBf2Uid",
        "outputId": "9395e50c-c0b9-46bd-89b8-66cf2678cd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        former versace store clerk sues over secret 'b...\n",
            "1        the 'roseanne' revival catches up to our thorn...\n",
            "2        mom starting to fear son's web series closest ...\n",
            "3        boehner just wants wife to listen  not come up...\n",
            "4        j k  rowling wishes snape happy birthday in th...\n",
            "                               ...                        \n",
            "26704                 american politics in moral free fall\n",
            "26705                              america's best    hikes\n",
            "26706                                reparations and obama\n",
            "26707    israeli ban targeting boycott supporters raise...\n",
            "26708                    gourmet gifts for the foodie     \n",
            "Name: headline, Length: 26709, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: TF-IDF + SVM\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "svm_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "print(\"=== TF-IDF + SVM ===\")\n",
        "print(classification_report(y_test, svm_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjJMYuQbvGl1",
        "outputId": "9e98c699-dc4d-4d48-8ac7-1f1974eea6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TF-IDF + SVM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.86      0.86      2996\n",
            "           1       0.82      0.81      0.81      2346\n",
            "\n",
            "    accuracy                           0.84      5342\n",
            "   macro avg       0.83      0.83      0.83      5342\n",
            "weighted avg       0.84      0.84      0.84      5342\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: Tokenization for Deep Learning Models\n",
        "vocab_size = 15000\n",
        "maxlen = 25\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=maxlen, padding='post')\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=maxlen, padding='post')\n"
      ],
      "metadata": {
        "id": "EjPnx9WMvKFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: CNN Model\n",
        "cnn_model = Sequential([\n",
        "    Embedding(vocab_size, 100, input_length=maxlen),\n",
        "    Conv1D(128, 3, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "    Conv1D(64, 3, activation='relu'),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train_seq, y_train, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\n",
        "\n",
        "cnn_pred = (cnn_model.predict(X_test_seq) > 0.5).astype(int)\n",
        "print(\"=== CNN ===\")\n",
        "print(classification_report(y_test, cnn_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhtZbv3FvOQc",
        "outputId": "8af817e3-6f3d-4aff-fd02-fb599c13b521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 51ms/step - accuracy: 0.6310 - loss: 0.6143 - val_accuracy: 0.8517 - val_loss: 0.3407\n",
            "Epoch 2/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.9082 - loss: 0.2383 - val_accuracy: 0.8549 - val_loss: 0.3365\n",
            "Epoch 3/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - accuracy: 0.9633 - loss: 0.1116 - val_accuracy: 0.8563 - val_loss: 0.3909\n",
            "Epoch 4/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 65ms/step - accuracy: 0.9863 - loss: 0.0465 - val_accuracy: 0.8470 - val_loss: 0.5185\n",
            "Epoch 5/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 48ms/step - accuracy: 0.9919 - loss: 0.0284 - val_accuracy: 0.8563 - val_loss: 0.6450\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "=== CNN ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86      2996\n",
            "           1       0.82      0.83      0.82      2346\n",
            "\n",
            "    accuracy                           0.84      5342\n",
            "   macro avg       0.84      0.84      0.84      5342\n",
            "weighted avg       0.84      0.84      0.84      5342\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: MHA-BiLSTM Model\n",
        "inputs = Input(shape=(maxlen,))\n",
        "x = Embedding(vocab_size, 100)(inputs)\n",
        "x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
        "x = LayerNormalization()(x + attn_output)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "mha_model = Model(inputs, outputs)\n",
        "mha_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "mha_model.fit(X_train_seq, y_train, epochs=5, batch_size=128, validation_split=0.1, verbose=1)\n",
        "\n",
        "mha_pred = (mha_model.predict(X_test_seq) > 0.5).astype(int)\n",
        "print(\"=== MHA-BiLSTM ===\")\n",
        "print(classification_report(y_test, mha_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JszWAoTWvOxe",
        "outputId": "8c285430-c5f3-4fcb-e8a2-033456e94467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 399ms/step - accuracy: 0.6869 - loss: 0.5548 - val_accuracy: 0.8615 - val_loss: 0.3272\n",
            "Epoch 2/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 390ms/step - accuracy: 0.9183 - loss: 0.2221 - val_accuracy: 0.8372 - val_loss: 0.3741\n",
            "Epoch 3/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 439ms/step - accuracy: 0.9534 - loss: 0.1328 - val_accuracy: 0.8554 - val_loss: 0.5282\n",
            "Epoch 4/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 382ms/step - accuracy: 0.9736 - loss: 0.0824 - val_accuracy: 0.8503 - val_loss: 0.5850\n",
            "Epoch 5/5\n",
            "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 390ms/step - accuracy: 0.9794 - loss: 0.0647 - val_accuracy: 0.8526 - val_loss: 0.6648\n",
            "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step\n",
            "=== MHA-BiLSTM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86      2996\n",
            "           1       0.84      0.78      0.81      2346\n",
            "\n",
            "    accuracy                           0.84      5342\n",
            "   macro avg       0.84      0.84      0.84      5342\n",
            "weighted avg       0.84      0.84      0.84      5342\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: BERT Fine-Tuning\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=40):\n",
        "    enc = tokenizer(\n",
        "        list(texts),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "    return enc\n",
        "\n",
        "train_enc = bert_encode(X_train, bert_tokenizer)\n",
        "test_enc = bert_encode(X_test, bert_tokenizer)\n",
        "\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "bert_model.compile(\n",
        "    optimizer=legacy.Adam(learning_rate=2e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "bert_model.fit(\n",
        "    train_enc['input_ids'], y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=2,\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "bert_preds = tf.argmax(bert_model.predict(test_enc['input_ids']).logits, axis=1).numpy()\n",
        "print(\"=== BERT ===\")\n",
        "print(classification_report(y_test, bert_preds))"
      ],
      "metadata": {
        "id": "NyjBoeO32P75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Comparative Analysis\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"TF-IDF+SVM\", \"CNN\", \"MHA-BiLSTM\", \"BERT\"],\n",
        "    \"Accuracy\": [\n",
        "        accuracy_score(y_test, svm_pred),\n",
        "        accuracy_score(y_test, cnn_pred),\n",
        "        accuracy_score(y_test, mha_pred),\n",
        "        accuracy_score(y_test, bert_preds)\n",
        "    ],\n",
        "    \"F1-Score\": [\n",
        "        f1_score(y_test, svm_pred),\n",
        "        f1_score(y_test, cnn_pred),\n",
        "        f1_score(y_test, mha_pred),\n",
        "        f1_score(y_test, bert_preds)\n",
        "    ]\n",
        "})\n",
        "print(results)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(results[\"Model\"], results[\"Accuracy\"], label='Accuracy', alpha=0.7)\n",
        "plt.bar(results[\"Model\"], results[\"F1-Score\"], label='F1', alpha=0.7)\n",
        "plt.title(\"Model Performance Comparison on Sarcasm Headlines Dataset\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CXWqqyRivUC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}